{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:33:57.145354Z",
     "start_time": "2023-09-05T16:33:54.007239Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statistics import mean\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import gca\n",
    "from matplotlib import figure\n",
    "import seaborn as sb\n",
    "\n",
    "import numba as nb\n",
    "from numba import njit\n",
    "from numba.core import types\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on packages...\n",
    "\n",
    "This analysis uses jsonlines to read history files, pandas & numpy for most data manipulation, matplotlib & seaborn for plotting, and numba to support fast nDCG calculations. It also makes use of the Python built-ins statistics and warnings. It has been tested with the following package versions (though it is not limited to these versions):\n",
    "* jsonlines 3.1.0\n",
    "* pandas 1.5.3\n",
    "* numpy 1.24.3\n",
    "* matplotlib 3.7.1\n",
    "* seaborn 0.12.2\n",
    "* numba 0.57.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate array for denom of ndcg calcs\n",
    "rec_list = 10\n",
    "base_logs = np.log2(np.arange(rec_list)+2)\n",
    "\n",
    "@njit\n",
    "def calculate_ndcg(histories, recommender, base_logs):\n",
    "    avg_of_ndcg = []\n",
    "    for history in histories:\n",
    "        i_count = 0\n",
    "        all_ndcg = 0\n",
    "        for user, items in histories[history].items():\n",
    "            scores = []\n",
    "            for item in items:\n",
    "                idx_array = np.asarray(recommender[user][1] == item).nonzero()[0]\n",
    "                if idx_array.size != 0:\n",
    "                    idx = idx_array[0]\n",
    "                    score = recommender[user][0][idx]\n",
    "                else:\n",
    "                    score = 0.0\n",
    "                scores.append(score)\n",
    "            scores = np.asarray(scores)\n",
    "            ideal_scores = np.sort(recommender[user][0])[::-1][:len(scores)]\n",
    "            scores[scores > 0] = 1.0\n",
    "            ideal_scores[ideal_scores > 0] = 1.0\n",
    "            recdcg = np.sum(scores/base_logs)\n",
    "            idealdcg = np.sum(ideal_scores/base_logs)\n",
    "            if idealdcg == 0.0:\n",
    "                ndcg = 0.0\n",
    "            else:\n",
    "                ndcg = recdcg/idealdcg\n",
    "            i_count += 1\n",
    "            all_ndcg += ndcg\n",
    "\n",
    "        avg_of_ndcg.append(all_ndcg/i_count)\n",
    "    return avg_of_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATISTICS FUNCTIONS ###\n",
    "\n",
    "# implicit type detection for pandas lookups\n",
    "def typecast(series, var):\n",
    "    dtype = pd.api.types.infer_dtype(series)\n",
    "    dtypes = {\"string\":str,\"integer\":int,\"floating\":float,\"mixed-integer-float\":float}\n",
    "    if type(var) != dtypes[dtype]:\n",
    "        var = dtypes[dtype](var)\n",
    "    if dtype not in dtypes.keys():\n",
    "        warnings.warn(\"Type of column \"+series.name+\" could not be implicitly determined. Defaulting to integer...\")\n",
    "        var = int(var)\n",
    "    return var\n",
    "\n",
    "# given an item id return a list of its features as binary values\n",
    "def get_item_features(item_features, item_id):\n",
    "    feature_values = []\n",
    "    for value in item_features.loc[(item_features.Item == typecast(item_features.Item, item_id))][\"BV\"]:\n",
    "        feature_values.append(value)\n",
    "    return feature_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VISUALIZATION FUNCTIONS ###\n",
    "\n",
    "def process_history(history, fair=True, compat=True, alloc=True, lists=True):\n",
    "    if fair:\n",
    "        fair_list = [entry['allocation']['fairness scores'] for entry in history]\n",
    "        fair_df = pd.DataFrame(fair_list)\n",
    "    else:\n",
    "        fair_df = None\n",
    "    if compat:\n",
    "        compat_list = [entry['allocation']['compatibility scores'] for entry in history]\n",
    "        compat_df = pd.DataFrame(compat_list)\n",
    "    else:\n",
    "        compat_df = None\n",
    "    if alloc:\n",
    "        alloc_list = [entry['allocation']['output'] for entry in history]\n",
    "        alloc_df = pd.DataFrame(alloc_list)\n",
    "        alloc_df['none'] = (alloc_df['COUNTRY_low_pfr'] == 0) & (alloc_df['loan_buck_5'] == 0)\n",
    "    else:\n",
    "        alloc_df = None\n",
    "    if lists:\n",
    "        results_list = [process_results(entry['choice_out']['results']) for entry in history]\n",
    "    else:\n",
    "        results_list = None\n",
    "    return fair_df, compat_df, alloc_df, results_list\n",
    "\n",
    "def process_results(result_structs):\n",
    "    return [(entry['item'], entry['score']) for entry in result_structs]\n",
    "\n",
    "def plot_fairness_time(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness\")\n",
    "    sb.lineplot(data=fair_df)\n",
    "    image_file = image_prefix + '-fairness.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_allocation(experiment_data, include_none=False, image_prefix=None):\n",
    "    alloc_df = pd.DataFrame(experiment_data[2])\n",
    "    if include_none is False:\n",
    "        if not alloc_df['none'][1:].any():\n",
    "            alloc_df.drop('none', axis=1, inplace=True)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Allocation\")\n",
    "    sb.lineplot(data=alloc_df.cumsum())\n",
    "    image_file = image_prefix + '-allocation.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_fairness_regret(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    regret = 1-fair_df\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness Regret\")\n",
    "    sb.lineplot(data=regret.cumsum())\n",
    "    image_file = image_prefix + '-regret.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def do_plots(experiment_data, include_none=False, image_prefix=None):\n",
    "    plot_fairness_time(experiment_data, include_none, image_prefix)\n",
    "    plot_allocation(experiment_data, include_none, image_prefix)\n",
    "    plot_fairness_regret(experiment_data, include_none, image_prefix)\n",
    "\n",
    "def process(experiment, include_none=False, image_prefix=None):\n",
    "    experiment_data = process_history(experiment)\n",
    "    do_plots(experiment_data, include_none, image_prefix)\n",
    "\n",
    "def process_names(name):\n",
    "    orig_name = name\n",
    "    for alloc in [\"Baseline\",\"Lottery\",\"Weighted Product\",\"Least Fair\"]:\n",
    "        name = name.replace(alloc, \"\")\n",
    "        if name != orig_name:\n",
    "            return name.rstrip(), alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Experiments\n",
    "Below is the code used to analyze experiments using our synthetically-generated dataset. It calculates per-experiment nDCG and adjusted proportional fairness, we use these metrics to create our scatterplots. It also tracks SCRUF's internal representations of fairness which we utilize to create boxplots of the system's fairness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify paths to json outputs from scruf\n",
    "BORDA_BASE = \"/data/history_file_baseline_s1.json\"\n",
    "BORDA_LOT = \"/data/history_file_product_alloc_borda_s1.json\"\n",
    "BORDA_WEIGHT = \"/data/history_file_weighted_alloc_borda_s1.json\"\n",
    "BORDA_LEAST = \"/data/history_file_least_fair_borda_s1.json\"\n",
    "\n",
    "COPELAND_BASE = \"/data/history_file_baseline_s1.json\"\n",
    "COPELAND_LOT = \"/data/history_file_product_alloc_copeland_s1.json\"\n",
    "COPELAND_WEIGHT = \"/data/history_file_weighted_alloc_copeland_s1.json\"\n",
    "COPELAND_LEAST = \"/data/history_file_least_fair_copeland_s1.json\"\n",
    "\n",
    "RP_BASE = \"/data/history_file_baseline_s1.json\"\n",
    "RP_LOT = \"/data/history_file_product_alloc_rankedpairs_s1.json\"\n",
    "RP_WEIGHT = \"/data/history_file_weighted_alloc_ranked_pairs_s1.json\"\n",
    "RP_LEAST = \"/data/history_file_least_fair_rankedpairs_s1.json\"\n",
    "\n",
    "WR_BASE = \"/data/history_file_baseline_s1.json\"\n",
    "WR_LEAST = \"/data/history_file_least_fair_rescore_s1.json\"\n",
    "WR_LOT = \"/data/history_file_product_alloc_rescore_s1.json\"\n",
    "WR_WEIGHT = \"/data/history_file_weighted_alloc_weighted_s1.json\"\n",
    "\n",
    "# paths to user|item|score files and item|feature files\n",
    "recs_file = \"/data/recs_s1.csv\"\n",
    "items_file = \"/data/items_s1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = pd.read_csv(recs_file, names=[\"User\",\"Item\",\"Score\"])\n",
    "item_features = pd.read_csv(items_file, names=[\"Item\",\"Feature\",\"BV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to change unless changing allocation/choice mechanism\n",
    "history_files = [BORDA_LOT, BORDA_WEIGHT, BORDA_LEAST, COPELAND_LEAST,COPELAND_LOT,COPELAND_WEIGHT,RP_LEAST,RP_LOT,RP_WEIGHT,WEIGHT_LEAST, WEIGHT_LOT,WEIGHT_WEIGHT,BASELINE]\n",
    "\n",
    "\n",
    "mechanisms = ['Borda Lottery', 'Borda Weighted Product',\n",
    "                  'Borda Least Fair','Copeland Least Fair',\n",
    "                  'Copeland Lottery', 'Copeland Weighted Product',\n",
    "                  'Ranked Pairs Least Fair',\n",
    "                  'Ranked Pairs Lottery', 'Ranked Pairs Weighted Product','Weighted Rescore Least Fair','Weighted Rescore Lottery',\n",
    "                  'Weighted Rescore Weighted Product', 'Baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from history files & store in dict\n",
    "processed_histories = {}\n",
    "for mechanism in mechanisms:\n",
    "    processed_histories[mechanism] = {}\n",
    "    processed_histories[mechanism][\"History\"] = []\n",
    "for history_file, mechanism in zip(history_files,mechanisms):\n",
    "    with jsonlines.open(history_file) as reader:\n",
    "        for obj in reader:\n",
    "            processed_histories[mechanism][\"History\"].append(obj)\n",
    "\n",
    "for mechanism in mechanisms:\n",
    "    processed_histories[mechanism][\"Statistics\"] = {}\n",
    "for mechanism in mechanisms:\n",
    "    for line in processed_histories[mechanism][\"History\"]:\n",
    "        results = line['choice_out']['results']\n",
    "        results_list = []\n",
    "        for result in results:\n",
    "            results_list.append(result['item'])\n",
    "        processed_histories[mechanism][\"Statistics\"][line['user']] = results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history + recommender info to typed dicts for numba\n",
    "light_histories = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.DictType(types.unicode_type, types.float64[:]),\n",
    ")\n",
    "for history in processed_histories:\n",
    "    consolidated_statistics = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.float64[:]\n",
    "    )\n",
    "    for user, items in processed_histories[history][\"Statistics\"].items():\n",
    "        consolidated_statistics[user] = np.asarray(items, dtype='f8')\n",
    "    light_histories[history] = consolidated_statistics\n",
    "\n",
    "light_recommender = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.UniTuple(types.float64[:], 2),\n",
    ")\n",
    "for user in recommender[\"User\"].unique():\n",
    "    scores = recommender[recommender[\"User\"] == user][\"Score\"].to_numpy(dtype='f8')\n",
    "    items = recommender[recommender[\"User\"] == user][\"Item\"].to_numpy(dtype='f8')\n",
    "    light_recommender[user] = (scores, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_of_ndcg = calculate_ndcg(light_histories, light_recommender, base_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe of average NDCG values\n",
    "ndcg_table = pd.DataFrame(data=avg_of_ndcg, index=mechanisms, columns=[\"NDCG\"])\n",
    "ndcg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates adj proportional fairness for the representation of item features\n",
    "num_features = 10\n",
    "feature_names = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "protected_features = [\"0\", \"1\"]\n",
    "fairness_targets = [0.25, 0.25]\n",
    "\n",
    "proportional_fairness = []\n",
    "adj_fairness = []\n",
    "for history, name in zip(processed_histories, mechanisms):\n",
    "    item_counter = 0\n",
    "    features = [0]*num_features\n",
    "    for items in processed_histories[history][\"Statistics\"].values():\n",
    "        for item in items:\n",
    "            item_counter += 1\n",
    "            for idx, val in enumerate(get_item_features(item_features, item)):\n",
    "                features[idx] = features[idx] + val\n",
    "    proportional = [x/item_counter for x in features]\n",
    "    proportional_fairness.append(proportional)\n",
    "    i = 0\n",
    "    calc_adj_fairness = []\n",
    "    for idx, name in enumerate(feature_names):\n",
    "        if name in protected_features:\n",
    "            fair_target = fairness_targets[i]\n",
    "            calc_adj_fairness.append(proportional[idx]/fair_target)\n",
    "            i = i+1\n",
    "    adj_fairness.append(calc_adj_fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_fairness_results = pd.DataFrame(data=adj_fairness, columns=protected_features, index=mechanisms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_fairness = prop_fairness_results.merge(ndcg_table, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fairness data to create boxplots\n",
    "# create df w/ allocation/choice mechs and fairness scores over time\n",
    "experiment_data = []\n",
    "for history in processed_histories:\n",
    "    choice, alloc = process_names(history)\n",
    "    experiment = pd.DataFrame(process_history(processed_histories[history][\"History\"])[0])\n",
    "    experiment[\"Choice Mech\"] = choice\n",
    "    experiment[\"Allocation Mech\"] = alloc\n",
    "    experiment_data.append(experiment)\n",
    "experiments = pd.concat(experiment_data)\n",
    "experiments[\"Time\"] = experiments.index\n",
    "experiments = pd.melt(experiments, id_vars=['Allocation Mech',\"Choice Mech\",\"Time\"], value_vars=[\"0\",\"1\"],var_name='Agent', value_name=\"Fairness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store baseline means, then remove baseline\n",
    "baseline1_mean = experiments[experiments[\"Allocation Mech\"] == \"Baseline\"][experiments[\"Agent\"] == \"0\"][\"Fairness\"].mean()\n",
    "baseline2_mean = experiments[experiments[\"Allocation Mech\"] == \"Baseline\"][experiments[\"Agent\"] == \"1\"][\"Fairness\"].mean()\n",
    "boxplots = experiments[experiments[\"Allocation Mech\"] != \"Baseline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for readability\n",
    "boxplots.loc[boxplots[\"Choice Mech\"] == \"Weighted Rescore\", \"Choice Mech\"] = \"Rescoring\"\n",
    "boxplots.loc[boxplots[\"Allocation Mech\"] == \"Weighted Product\", \"Allocation Mech\"] = \"Weighted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate FacetGrid boxplots of internal representations of fairness scores \n",
    "sb.set(font_scale=1.5)\n",
    "\n",
    "g = sb.catplot(data=boxplots,\n",
    "           row=\"Allocation Mech\",\n",
    "           col=\"Choice Mech\",\n",
    "           x=\"Fairness\",\n",
    "           y=\"Agent\",\n",
    "           order=[\"0\",\"1\"],\n",
    "           kind=\"box\",\n",
    "           height=2,\n",
    "           aspect=2,\n",
    "           margin_titles=True)\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.axvline(x=baseline1_mean, color='tab:blue', ls=\"dashed\", lw=3)\n",
    "    ax.axvline(x=baseline2_mean, color='tab:orange', ls=\"dashed\", lw=3)\n",
    "\n",
    "g.set_titles(row_template='{row_name}', col_template='{col_name}')\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = ndcg_fairness.loc[\"Baseline\"]\n",
    "ndcg_fairness = ndcg_fairness.drop(\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for readability\n",
    "ndcg_fairness['Choice'] = ('Borda', 'Borda', 'Borda', 'Copeland', 'Copeland', 'Copeland', 'Ranked Pairs', 'Ranked Pairs', 'Ranked Pairs', 'Rescoring', 'Rescoring', 'Rescoring')\n",
    "ndcg_fairness['Allocation'] = ('Lottery', 'Weighted', 'Least Fair', 'Least Fair', 'Lottery', 'Weighted', 'Least Fair', 'Lottery', 'Weighted', 'Least Fair', 'Lottery', 'Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_fairness[\"Average Fairness Score\"] = ndcg_fairness.apply(lambda x: (x[\"COUNTRY_low_pfr\"]+x[\"loan_buck_5\"])/2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scatterplot(fairness_df, base_ndcg, filename=None):\n",
    "    sb.set_style(\"white\")\n",
    "    plot = sb.scatterplot(x='Average Fairness Score',\n",
    "                      y='NDCG',\n",
    "                      data=fairness_df,\n",
    "                      style=\"Allocation\",\n",
    "                      hue=\"Choice\",\n",
    "                      s=100,\n",
    "                      markers={\"Lottery\": \"^\", \"Weighted\": \"X\", \"Least Fair\": \"o\"})\n",
    "\n",
    "    sb.set(font_scale = 1.25)\n",
    "    plt.axhline(y = base_ndcg, linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Average Fairness Score\", fontsize=12, labelpad=10)\n",
    "    plt.ylabel(\"nDCG\", fontsize=12, labelpad=11)\n",
    "    plt.legend(loc=\"lower left\", fontsize=12) # location for plots in paper, may need to change with other data\n",
    "    plt.tight_layout()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_scatterplot(ndcg_fairness, baseline_accuracy, filename='synthetic_scatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Kiva Experiments\n",
    "Below is the code used to analyze experiments using the Kiva dataset. It mirrors the above analysis, with a few small changes to support the Kiva dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:33:57.181920Z",
     "start_time": "2023-09-05T16:33:57.145824Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify paths to json outputs from scruf\n",
    "BORDA_LOT = \"/data/history_file_product_lottery_borda_kiva.json\"\n",
    "BORDA_WEIGHT = \"/data/history_file_weighted_alloc_borda_kiva.json\"\n",
    "BORDA_LEAST = \"/data/history_file_least_fair_borda_kiva.json\"\n",
    "\n",
    "\n",
    "COPELAND_LOT = \"/data/history_file_product_lottery_copeland_kiva.json\"\n",
    "COPELAND_WEIGHT = \"/data/history_file_weighted_alloc_copeland_kiva.json\"\n",
    "COPELAND_LEAST = \"/data/history_file_least_fair_copeland_kiva.json\"\n",
    "\n",
    "\n",
    "RP_LOT = \"/data/history_file_product_lottery_rankedpairs_kiva.json\"\n",
    "RP_WEIGHT = \"/data/history_file_weighted_alloc_rankedpairs_kiva.json\"\n",
    "RP_LEAST = \"/data/history_file_least_fair_rankedpairs_kiva.json\"\n",
    "\n",
    "WEIGHT_LEAST = \"/data/history_file_least_fair_weighted_kiva.json\"\n",
    "WEIGHT_LOT = \"/data/history_file_product_lottery_rescore_kiva.json\"\n",
    "WEIGHT_WEIGHT = \"/data/history_file_weighted_alloc_weighted_kiva.json\"\n",
    "\n",
    "BASELINE = \"/data/history_file_baseline_kiva.json\"\n",
    "\n",
    "# paths to user|item|score files and item|feature files\n",
    "recs_file = \"/data/kiva_recs.csv\"\n",
    "items_file = \"/data/kiva_item.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:33:57.292218Z",
     "start_time": "2023-09-05T16:33:57.146297Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommender = pd.read_csv(recs_file, names=[\"User\",\"Item\",\"Score\"])\n",
    "item_features = pd.read_csv(items_file, names=[\"Item\",\"Feature\",\"BV\"])\n",
    "item_features = item_features[(item_features[\"Feature\"]== 'COUNTRY_low_pfr') | (item_features['Feature'] == 'loan_buck_5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:34:15.999931Z",
     "start_time": "2023-09-05T16:33:57.711976Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = item_features\n",
    "\n",
    "# create a list of all feature combinations\n",
    "all_features = [\"COUNTRY_low_pfr\", \"loan_buck_5\"]\n",
    "\n",
    "# iterate over each item ID and feature combination\n",
    "for item_id in range(1, 4998):\n",
    "    for feature in all_features:\n",
    "        # check if the row exists\n",
    "        row_exists = ((dataset[\"Item\"] == item_id) & (dataset[\"Feature\"] == feature)).any()\n",
    "\n",
    "        # if the row doesn't exist, add a new row with item ID, feature, and value of 0\n",
    "        if not row_exists:\n",
    "            dataset = dataset.append({\"Item\": item_id, \"Feature\": feature, \"BV\": 0}, ignore_index=True)\\\n",
    "\n",
    "item_features = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:34:16.000201Z",
     "start_time": "2023-09-05T16:34:15.824869Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no need to change unless changing allocation/choice mechanism\n",
    "history_files = [BORDA_LOT, BORDA_WEIGHT, BORDA_LEAST, COPELAND_LEAST,COPELAND_LOT,COPELAND_WEIGHT,RP_LEAST,RP_LOT,RP_WEIGHT,WEIGHT_LEAST, WEIGHT_LOT,WEIGHT_WEIGHT,BASELINE]\n",
    "\n",
    "\n",
    "mechanisms = ['Borda Lottery', 'Borda Weighted Product',\n",
    "                  'Borda Least Fair','Copeland Least Fair',\n",
    "                  'Copeland Lottery', 'Copeland Weighted Product',\n",
    "                  'Ranked Pairs Least Fair',\n",
    "                  'Ranked Pairs Lottery', 'Ranked Pairs Weighted Product','Weighted Rescore Least Fair','Weighted Rescore Lottery',\n",
    "                  'Weighted Rescore Weighted Product', 'Baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:34:24.710479Z",
     "start_time": "2023-09-05T16:34:15.827972Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in data from history files & store in dict\n",
    "processed_histories = {}\n",
    "for mechanism in mechanisms:\n",
    "    processed_histories[mechanism] = {}\n",
    "    processed_histories[mechanism][\"History\"] = []\n",
    "for history_file, mechanism in zip(history_files,mechanisms):\n",
    "    with jsonlines.open(history_file) as reader:\n",
    "        for obj in reader:\n",
    "            processed_histories[mechanism][\"History\"].append(obj)\n",
    "\n",
    "for mechanism in mechanisms:\n",
    "    processed_histories[mechanism][\"Statistics\"] = {}\n",
    "for mechanism in mechanisms:\n",
    "    for line in processed_histories[mechanism][\"History\"]:\n",
    "        results = line['choice_out']['results']\n",
    "        results_list = []\n",
    "        for result in results:\n",
    "            results_list.append(result['item'])\n",
    "        processed_histories[mechanism][\"Statistics\"][line['user']] = results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:36:44.834420Z",
     "start_time": "2023-09-05T16:34:24.721884Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert history + recommender info to typed dicts for numba\n",
    "light_histories = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.DictType(types.unicode_type, types.float64[:]),\n",
    ")\n",
    "for history in processed_histories:\n",
    "    consolidated_statistics = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.float64[:]\n",
    "    )\n",
    "    for user, items in processed_histories[history][\"Statistics\"].items():\n",
    "        consolidated_statistics[user] = np.asarray(items, dtype='f8')\n",
    "    light_histories[history] = consolidated_statistics\n",
    "\n",
    "light_recommender = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.UniTuple(types.float64[:], 2),\n",
    ")\n",
    "for user in recommender[\"User\"].unique():\n",
    "    scores = recommender[recommender[\"User\"] == user][\"Score\"].to_numpy(dtype='f8')\n",
    "    items = recommender[recommender[\"User\"] == user][\"Item\"].to_numpy(dtype='f8')\n",
    "    light_recommender[user] = (scores, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:36:49.057839Z",
     "start_time": "2023-09-05T16:36:44.869987Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_of_ndcg = calculate_ndcg(light_histories, light_recommender, base_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:36:49.070947Z",
     "start_time": "2023-09-05T16:36:49.051712Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates dataframe of average NDCG values\n",
    "ndcg_table = pd.DataFrame(data=avg_of_ndcg, index=mechanisms, columns=[\"NDCG\"])\n",
    "ndcg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:41:28.350827Z",
     "start_time": "2023-09-05T16:38:40.118111Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculates adj proportional fairness for the representation of item features\n",
    "num_features = 10\n",
    "feature_names = [\"COUNTRY_low_pfr\", \"loan_buck_5\"]\n",
    "protected_features = [\"COUNTRY_low_pfr\", \"loan_buck_5\"]\n",
    "fairness_targets = [0.20, 0.30]\n",
    "\n",
    "proportional_fairness = []\n",
    "adj_fairness = []\n",
    "for history, name in zip(processed_histories, mechanisms):\n",
    "    item_counter = 0\n",
    "    features = [0]*num_features\n",
    "    for items in processed_histories[history][\"Statistics\"].values():\n",
    "        for item in items:\n",
    "            item_counter += 1\n",
    "            for idx, val in enumerate(get_item_features(item_features, item)):\n",
    "                features[idx] = features[idx] + val\n",
    "    proportional = [x/item_counter for x in features]\n",
    "    proportional_fairness.append(proportional)\n",
    "    i = 0\n",
    "    calc_adj_fairness = []\n",
    "    for idx, name in enumerate(feature_names):\n",
    "        if name in protected_features:\n",
    "            fair_target = fairness_targets[i]\n",
    "            calc_adj_fairness.append(proportional[idx]/fair_target)\n",
    "            i = i+1\n",
    "    adj_fairness.append(calc_adj_fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:41:28.357041Z",
     "start_time": "2023-09-05T16:41:28.355391Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prop_fairness_results = pd.DataFrame(data=adj_fairness, columns=protected_features, index=mechanisms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:41:28.365905Z",
     "start_time": "2023-09-05T16:41:28.361903Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_fairness = prop_fairness_results.merge(ndcg_table, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:47:42.128240Z",
     "start_time": "2023-09-05T19:47:37.787588Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get fairness data to create boxplots\n",
    "# create df w/ allocation/choice mechs and fairness scores over time\n",
    "experiment_data = []\n",
    "for history in processed_histories:\n",
    "    choice, alloc = process_names(history)\n",
    "    experiment = pd.DataFrame(process_history(processed_histories[history][\"History\"])[0])\n",
    "    experiment[\"Choice Mech\"] = choice\n",
    "    experiment[\"Allocation Mech\"] = alloc\n",
    "    experiment_data.append(experiment)\n",
    "experiments = pd.concat(experiment_data)\n",
    "experiments[\"Time\"] = experiments.index\n",
    "experiments = pd.melt(experiments, id_vars=['Allocation Mech',\"Choice Mech\",\"Time\"], value_vars=[\"COUNTRY_low_pfr\",\"loan_buck_5\"],var_name='Agent', value_name=\"Fairness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:47:46.328898Z",
     "start_time": "2023-09-05T19:47:46.129748Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store baseline means, then remove baseline\n",
    "baseline1_mean = experiments[experiments[\"Allocation Mech\"] == \"Baseline\"][experiments[\"Agent\"] == \"COUNTRY_low_pfr\"][\"Fairness\"].mean()\n",
    "baseline2_mean = experiments[experiments[\"Allocation Mech\"] == \"Baseline\"][experiments[\"Agent\"] == \"loan_buck_5\"][\"Fairness\"].mean()\n",
    "boxplots = experiments[experiments[\"Allocation Mech\"] != \"Baseline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:47:51.570065Z",
     "start_time": "2023-09-05T19:47:51.436866Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename for readability\n",
    "boxplots.loc[boxplots[\"Choice Mech\"] == \"Weighted Rescore\", \"Choice Mech\"] = \"Rescoring\"\n",
    "boxplots.loc[boxplots[\"Allocation Mech\"] == \"Weighted Product\", \"Allocation Mech\"] = \"Weighted\"\n",
    "boxplots.replace(\"COUNTRY_low_pfr\", \"Country\", inplace=True)\n",
    "boxplots.replace(\"loan_buck_5\", \"Loan Size\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:47:58.352211Z",
     "start_time": "2023-09-05T19:47:53.747004Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate FacetGrid boxplots of internal representations of fairness scores \n",
    "sb.set(font_scale=1.5)\n",
    "\n",
    "g = sb.catplot(data=boxplots,\n",
    "           row=\"Allocation Mech\",\n",
    "           col=\"Choice Mech\",\n",
    "           x=\"Fairness\",\n",
    "           y=\"Agent\",\n",
    "           order=[\"Country\",\"Loan Size\"],\n",
    "           kind=\"box\",\n",
    "           height=2,\n",
    "           aspect=2,\n",
    "           margin_titles=True)\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.axvline(x=baseline1_mean, color='tab:blue', ls=\"dashed\", lw=3)\n",
    "    ax.axvline(x=baseline2_mean, color='tab:orange', ls=\"dashed\", lw=3)\n",
    "\n",
    "g.set_titles(row_template='{row_name}', col_template='{col_name}')\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:48:39.497489Z",
     "start_time": "2023-09-05T19:48:39.446774Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:50:19.174205Z",
     "start_time": "2023-09-05T19:50:19.107630Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_accuracy = ndcg_fairness.loc[\"Baseline\"]\n",
    "ndcg_fairness = ndcg_fairness.drop(\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:50:22.296072Z",
     "start_time": "2023-09-05T19:50:22.240063Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename for readability\n",
    "ndcg_fairness['Choice'] = ('Borda', 'Borda', 'Borda', 'Copeland', 'Copeland', 'Copeland', 'Ranked Pairs', 'Ranked Pairs', 'Ranked Pairs', 'Rescoring', 'Rescoring', 'Rescoring')\n",
    "ndcg_fairness['Allocation'] = ('Lottery', 'Weighted', 'Least Fair', 'Least Fair', 'Lottery', 'Weighted', 'Least Fair', 'Lottery', 'Weighted', 'Least Fair', 'Lottery', 'Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:50:27.574880Z",
     "start_time": "2023-09-05T19:50:27.530920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_fairness[\"Average Fairness Score\"] = ndcg_fairness.apply(lambda x: (x[\"COUNTRY_low_pfr\"]+x[\"loan_buck_5\"])/2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:50:30.612985Z",
     "start_time": "2023-09-05T19:50:30.566037Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_scatterplot(fairness_df, base_ndcg, filename=None):\n",
    "    sb.set_style(\"white\")\n",
    "    plot = sb.scatterplot(x='Average Fairness Score',\n",
    "                      y='NDCG',\n",
    "                      data=fairness_df,\n",
    "                      style=\"Allocation\",\n",
    "                      hue=\"Choice\",\n",
    "                      s=100,\n",
    "                      markers={\"Lottery\": \"^\", \"Weighted\": \"X\", \"Least Fair\": \"o\"})\n",
    "\n",
    "    sb.set(font_scale = 1.25)\n",
    "    plt.axhline(y = base_ndcg, linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Average Fairness Score\", fontsize=12, labelpad=10)\n",
    "    plt.ylabel(\"nDCG\", fontsize=12, labelpad=11)\n",
    "    plt.legend(loc=\"lower left\", fontsize=12) # location for plots in paper, may need to change with other data\n",
    "    plt.tight_layout()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T19:50:38.913894Z",
     "start_time": "2023-09-05T19:50:31.600965Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_scatterplot(ndcg_fairness, baseline_accuracy, filename='kiva_scatter.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
